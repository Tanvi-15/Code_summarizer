{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train an Encoder-Decoder Model for Java Code Summarisation\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uxC3BW-C-ixH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSTALL LIBRARIES**\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------"
      ],
      "metadata": {
        "id": "pvVjlMmx-5GM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71856a92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f75aacd-0035-4097-bce3-8079fabe03ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install transformers datasets evaluate rouge_score bert_score --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MOUNT DRIVE**\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------"
      ],
      "metadata": {
        "id": "8i-bWJ8IaR2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# ==========================\n",
        "# Mount Google Drive\n",
        "# ==========================\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0uA4xCATOj_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d18c58f-d708-44b1-cb31-831f6d89b2c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOKENIZATION**\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------"
      ],
      "metadata": {
        "id": "GaZ7DTW93HJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import re\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset and tokenizer\n",
        "dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"java\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\")\n",
        "MAX_SOURCE_LENGTH = 256\n",
        "MAX_TARGET_LENGTH = 80\n",
        "\n",
        "class CodeSummaryDataset(Dataset):\n",
        "    def __init__(self, hf_dataset_split):\n",
        "        self.dataset = hf_dataset_split\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.dataset[idx]\n",
        "        code = sample[\"code\"]\n",
        "        summary = sample[\"docstring\"]\n",
        "\n",
        "        # Normalize whitespace\n",
        "        code = re.sub(r'\\s+', ' ', code).strip()\n",
        "        summary = re.sub(r'\\s+', ' ', summary).strip()\n",
        "\n",
        "        # Tokenize code (encoder input)\n",
        "        source = tokenizer(code,\n",
        "                           padding=\"max_length\",\n",
        "                           truncation=True,\n",
        "                           max_length=MAX_SOURCE_LENGTH,\n",
        "                           return_tensors=\"pt\",\n",
        "                           add_special_tokens=True)\n",
        "\n",
        "        # Tokenize summary (decoder input/labels)\n",
        "        target = tokenizer(summary,\n",
        "                           padding=\"max_length\",\n",
        "                           truncation=True,\n",
        "                           max_length=MAX_TARGET_LENGTH,\n",
        "                           return_tensors=\"pt\",\n",
        "                           add_special_tokens=True)\n",
        "\n",
        "        # Shift for decoder input vs labels\n",
        "        decoder_input_ids = target.input_ids[:, :-1]\n",
        "        labels = target.input_ids[:, 1:]\n",
        "\n",
        "        # Pad to max target length - 1 (after shift)\n",
        "        pad_len = MAX_TARGET_LENGTH - 1 - decoder_input_ids.size(1)\n",
        "        if pad_len > 0:\n",
        "            pad = torch.full((1, pad_len), tokenizer.pad_token_id)\n",
        "            decoder_input_ids = torch.cat([decoder_input_ids, pad], dim=1)\n",
        "            labels = torch.cat([labels, pad], dim=1)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": source.input_ids.squeeze(0),\n",
        "            \"decoder_input_ids\": decoder_input_ids.squeeze(0),\n",
        "            \"labels\": labels.squeeze(0)\n",
        "        }"
      ],
      "metadata": {
        "id": "N4wnOMHwZ9F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL** **ARCHITECTURE**\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------"
      ],
      "metadata": {
        "id": "4x6Atr041qla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ”§ Custom Encoder-Decoder Model using PyTorch and CodeT5 Embeddings\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import time\n",
        "import os\n",
        "from google.colab import drive\n",
        "import re\n",
        "import math\n",
        "\n",
        "# ==========================\n",
        "# Sinusoidal Positional Encoding\n",
        "# ==========================\n",
        "def get_sinusoidal_encoding(max_len, d_model):\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    return pe.unsqueeze(0)  # [1, max_len, d_model]\n",
        "\n",
        "# ==========================\n",
        "# Model Components\n",
        "# ==========================\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        B, T, D = query.size()\n",
        "        H = self.num_heads\n",
        "\n",
        "        def reshape(x):\n",
        "            return x.view(B, -1, H, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        Q = reshape(self.q_proj(query))\n",
        "        K = reshape(self.k_proj(key))\n",
        "        V = reshape(self.v_proj(value))\n",
        "\n",
        "        attn_weights = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).unsqueeze(2)  # [B, 1, 1, T]\n",
        "            attn_weights = attn_weights.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn_probs = F.softmax(attn_weights, dim=-1)\n",
        "        attn_output = torch.matmul(attn_probs, V)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, D)\n",
        "        return self.out_proj(attn_output)\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        attn_output = self.self_attn(src, src, src, src_mask)\n",
        "        src = self.norm1(src + self.dropout(attn_output))\n",
        "        ff_output = self.ff(src)\n",
        "        src = self.norm2(src + self.dropout(ff_output))\n",
        "        return src\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.norm3 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask, memory_mask):\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
        "        tgt = self.norm1(tgt + self.dropout(tgt2))\n",
        "        tgt2 = self.cross_attn(tgt, memory, memory, memory_mask)\n",
        "        tgt = self.norm2(tgt + self.dropout(tgt2))\n",
        "        tgt2 = self.ff(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout(tgt2))\n",
        "        return tgt\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, embed_dim=768, num_heads=8, ff_dim=2048, num_layers=4, dropout=0.1, vocab_size=32100, max_len=512):\n",
        "        super().__init__()\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            TransformerDecoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "        self.positional_encoding = get_sinusoidal_encoding(max_len, embed_dim)\n",
        "\n",
        "    def forward(self, encoder_embeddings, decoder_embeddings, src_mask=None, tgt_mask=None):\n",
        "        B, S, _ = encoder_embeddings.size()\n",
        "        B2, T, _ = decoder_embeddings.size()\n",
        "\n",
        "        # Add sinusoidal positional embeddings\n",
        "        encoder_embeddings = encoder_embeddings + self.positional_encoding[:, :S, :].to(encoder_embeddings.device)\n",
        "        decoder_embeddings = decoder_embeddings + self.positional_encoding[:, :T, :].to(decoder_embeddings.device)\n",
        "\n",
        "        memory = encoder_embeddings\n",
        "        for layer in self.encoder_layers:\n",
        "            memory = layer(memory, src_mask)\n",
        "\n",
        "        if tgt_mask is None:\n",
        "            tgt_mask = torch.tril(torch.ones(T, T)).to(decoder_embeddings.device)  # [T, T]\n",
        "            tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, T, T]\n",
        "\n",
        "        output = decoder_embeddings\n",
        "        for layer in self.decoder_layers:\n",
        "            output = layer(output, memory, tgt_mask, src_mask)\n",
        "\n",
        "        logits = self.lm_head(output)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "ip5KsUsA1j5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MODEL TRAINING**\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------"
      ],
      "metadata": {
        "id": "M0nQWF1I5OoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Load dataset and tokenizer\n",
        "dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"java\")\n",
        "train_dataset = CodeSummaryDataset(dataset[\"train\"])\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Load tokenizer and embeddings\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\")\n",
        "codet5 = AutoModel.from_pretrained(\"Salesforce/codet5-base\")\n",
        "embedding_layer = codet5.get_input_embeddings()\n",
        "\n",
        "# Training Setup\n",
        "model = TransformerModel()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "embedding_layer.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "\n",
        "EPOCHS = 3\n",
        "CHECKPOINT_DIR = \"/content/checkpoints\"\n",
        "DRIVE_DIR = \"/content/drive/MyDrive/codet5_checkpoints-new\"\n",
        "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "\n",
        "total_batches = len(train_loader)\n",
        "save_every = total_batches // 2\n",
        "\n",
        "print(f\" Starting training for {EPOCHS} epochs â€” {total_batches} batches/epoch\\n\")\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    print(f\" Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        start_time = time.time()\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        decoder_input_ids = batch[\"decoder_input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoder_input_embeddings = embedding_layer(input_ids)\n",
        "            decoder_input_embeddings = embedding_layer(decoder_input_ids)\n",
        "\n",
        "        logits = model(encoder_input_embeddings, decoder_input_embeddings)\n",
        "        loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        step_time = time.time() - start_time\n",
        "        print(f\" Epoch {epoch+1} | Batch {batch_idx+1}/{total_batches} | Loss: {loss.item():.4f} | Time: {step_time:.2f}s\")\n",
        "\n",
        "        # Checkpoint mid-epoch\n",
        "        if (batch_idx + 1) % save_every == 0:\n",
        "            name = f\"model_epoch{epoch+1}_step{batch_idx+1}.pt\"\n",
        "            torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, name))\n",
        "            torch.save(model.state_dict(), os.path.join(DRIVE_DIR, name))\n",
        "            print(f\" Checkpoint saved to Colab and Drive: {name}\")\n",
        "\n",
        "    # End-of-epoch checkpoint\n",
        "    final_name = f\"model_epoch{epoch+1}.pt\"\n",
        "    torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, final_name))\n",
        "    torch.save(model.state_dict(), os.path.join(DRIVE_DIR, final_name))\n",
        "    print(f\"End-of-epoch checkpoint saved: {final_name}\")\n",
        "    print(f\"Average Epoch Loss: {total_loss / total_batches:.4f}\\\\n\")\n"
      ],
      "metadata": {
        "id": "7SpLZm695cxY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INFERENCE PIPELINE**\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------"
      ],
      "metadata": {
        "id": "0gBB-C8tZF7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSummaryGenerator:\n",
        "    def __init__(self, model, tokenizer, embedding_layer, device, max_input_len=256, max_target_len=80):\n",
        "        self.model = model.to(device)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embedding_layer = embedding_layer.to(device)\n",
        "        self.device = device\n",
        "        self.max_input_len = max_input_len\n",
        "        self.max_target_len = max_target_len\n",
        "\n",
        "    def top_k_sampling(self, logits, k=50, temperature=1.0):\n",
        "        logits = logits / temperature\n",
        "        top_k_values, top_k_indices = torch.topk(logits, k, dim=-1)\n",
        "        probs = torch.softmax(top_k_values, dim=-1)\n",
        "        sampled_idx = torch.multinomial(probs, num_samples=1)\n",
        "        return top_k_indices.gather(-1, sampled_idx)\n",
        "\n",
        "    def generate_summary(self, code_snippet):\n",
        "        import re\n",
        "        code_snippet = re.sub(r'\\s+', ' ', code_snippet).strip()\n",
        "        source = self.tokenizer(code_snippet,\n",
        "                                padding=\"max_length\",\n",
        "                                truncation=True,\n",
        "                                max_length=self.max_input_len,\n",
        "                                return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoder_embeddings = self.embedding_layer(source[\"input_ids\"])\n",
        "            memory = encoder_embeddings\n",
        "            for layer in self.model.encoder_layers:\n",
        "                memory = layer(memory, source[\"attention_mask\"])\n",
        "\n",
        "        generated_ids = torch.full((1, 1), self.tokenizer.pad_token_id, dtype=torch.long).to(self.device)\n",
        "\n",
        "        for _ in range(self.max_target_len):\n",
        "            with torch.no_grad():\n",
        "                decoder_embeddings = self.embedding_layer(generated_ids)\n",
        "                logits = self.model(memory, decoder_embeddings)\n",
        "                next_token_logits = logits[:, -1, :]\n",
        "                # To top-k sampling:\n",
        "                next_token_id = self.top_k_sampling(next_token_logits, k=50, temperature=0.7)\n",
        "                generated_ids = torch.cat((generated_ids, next_token_id), dim=1)\n",
        "\n",
        "                if next_token_id.item() == self.tokenizer.pad_token_id:\n",
        "                    break\n",
        "                if (generated_ids[0, -10:] == next_token_id).all():\n",
        "                    print(\"Stopping early due to repetition\")\n",
        "                    break\n",
        "\n",
        "        return self.tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()\n"
      ],
      "metadata": {
        "id": "yKVGRVQbZ0mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Load tokenizer and embeddings\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\")\n",
        "codet5 = AutoModel.from_pretrained(\"Salesforce/codet5-base\")\n",
        "embedding_layer = codet5.get_input_embeddings()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "checkpoint_path = hf_hub_download(\n",
        "    repo_id=\"pritammane105/Custom-Java-Summarisation\",\n",
        "    filename=\"my_model.pt\"\n",
        ")\n",
        "\n",
        "model = TransformerModel()\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "model.to(device)\n",
        "embedding_layer.to(device)\n",
        "model.eval()\n",
        "\n",
        "generator = CustomSummaryGenerator(model, tokenizer, embedding_layer, device)\n",
        "summary = generator.generate_summary(\"public int add(int a, int b) { return a + b; }\")\n",
        "print(\"Summary: \", summary)\n"
      ],
      "metadata": {
        "id": "0BCGmfJSdd5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INFERENCE ON VALIDATION & TEST SETS**\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------"
      ],
      "metadata": {
        "id": "oQq9WEloAIlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def generate_batch_summaries(generator, split: str, save_path: str):\n",
        "    dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"java\")[split]\n",
        "\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    print(f\"Generating summaries for the {split} set...\")\n",
        "    for idx, example in enumerate(dataset):\n",
        "        code = example[\"code\"]\n",
        "        reference = example[\"docstring\"]\n",
        "\n",
        "        try:\n",
        "            summary = generator.generate_summary(code)\n",
        "        except Exception as e:\n",
        "            print(f\"Error at index {idx}: {e}\")\n",
        "            summary = \"\"\n",
        "\n",
        "        predictions.append(summary)\n",
        "        references.append(reference)\n",
        "\n",
        "        if (idx + 1) % 50 == 0:\n",
        "            print(f\"Processed {idx + 1}/{len(dataset)} examples\")\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"gold_summary\": references,\n",
        "        \"predicted_summary\": predictions\n",
        "    })\n",
        "\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    df.to_csv(save_path, index=False)\n",
        "    print(f\"Saved {split} summaries to: {save_path}\")\n"
      ],
      "metadata": {
        "id": "2VBWFITsgyWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run for validation set\n",
        "generate_batch_summaries(generator, \"validation\", \"/content/drive/MyDrive/custom_val_topk_predictions.csv\")\n",
        "\n",
        "# Run for test set\n",
        "generate_batch_summaries(generator, \"test\", \"/content/drive/MyDrive/custom_test_topk_predictions.csv\")\n"
      ],
      "metadata": {
        "id": "ImLji5d3iS_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "java_code = \"\"\"\n",
        "public int max(int a, int b) {\n",
        "    return a > b ? a : b;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "summary = generate_summary(java_code)\n",
        "print(\"Generated Summary:\", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjgGIdKZIP4C",
        "outputId": "81ee0900-9767-467b-9d10-e0724eba6c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ Stopping early due to repetition\n",
            "ðŸ§  Generated Summary: FixFixFixFixFixFixFixFixFixFix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "java_code = \"\"\"\n",
        "public int function(int a, int b) {\n",
        "    return a > b ? a : b;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "summary = generate_summary(java_code)\n",
        "print(\"Generated Summary:\", summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIilqDCOsErs",
        "outputId": "c14b9a38-5fc5-434a-fd6c-52924ac87373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš ï¸ Stopping early due to repetition\n",
            "ðŸ§  Generated Summary: FixFixFixFixFixFixFixFixFixFix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EVALUATION**\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------"
      ],
      "metadata": {
        "id": "xNDjiHJbvExS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "class SummaryEvaluator:\n",
        "    def __init__(self):\n",
        "        self.rouge = evaluate.load(\"rouge\")\n",
        "        self.bleu = evaluate.load(\"bleu\")\n",
        "        self.bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "    def avg_token_repetition(self, predictions):\n",
        "        \"\"\"\n",
        "        Computes the average number of repeated tokens per prediction.\n",
        "        A high repetition score indicates redundancy in the generated text.\n",
        "        \"\"\"\n",
        "        rep_counts = []\n",
        "        for text in predictions:\n",
        "            tokens = text.strip().split()\n",
        "            counts = Counter(tokens)\n",
        "            repeated_tokens = sum(v for v in counts.values() if v > 1)\n",
        "            rep_counts.append(repeated_tokens / max(1, len(tokens)))\n",
        "        return np.mean(rep_counts)\n",
        "\n",
        "    def evaluate_csvs(self, files: Dict[str, str]):\n",
        "        \"\"\"\n",
        "        Evaluates multiple prediction files and returns a DataFrame of metrics.\n",
        "        Each file must be a CSV with columns: 'predicted_summary' and 'gold_summary'.\n",
        "        \"\"\"\n",
        "        all_results = []\n",
        "        for name, path in files.items():\n",
        "            if not os.path.exists(path):\n",
        "                print(f\"File not found: {path}\")\n",
        "                continue\n",
        "\n",
        "            df = pd.read_csv(path)\n",
        "            predictions = df[\"predicted_summary\"].astype(str).tolist()\n",
        "            references = df[\"gold_summary\"].astype(str).tolist()\n",
        "\n",
        "            rouge_scores = self.rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
        "            bleu_score = self.bleu.compute(predictions=predictions, references=references)\n",
        "            bert_score = self.bertscore.compute(predictions=predictions, references=references, lang=\"en\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            repetition = self.avg_token_repetition(predictions)\n",
        "\n",
        "            all_results.append({\n",
        "                \"Version\": name,\n",
        "                \"ROUGE-1\": round(rouge_scores[\"rouge1\"], 4),\n",
        "                \"ROUGE-2\": round(rouge_scores[\"rouge2\"], 4),\n",
        "                \"ROUGE-L\": round(rouge_scores[\"rougeL\"], 4),\n",
        "                \"BLEU\": round(bleu_score[\"bleu\"], 4),\n",
        "                \"BERTScore\": round(np.mean(bert_score[\"f1\"]), 4),\n",
        "                \"Avg Token Repetition\": round(repetition, 4)\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(all_results)\n"
      ],
      "metadata": {
        "id": "-cxnNWI8_YrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Evaluation\n",
        "    evaluator = SummaryEvaluator()\n",
        "    files = {\n",
        "        \"custom_topk_val\": \"/content/drive/MyDrive/custom_val_topk_predictions.csv\",\n",
        "        \"custom_topk_text\": \"/content/drive/MyDrive/custom_test_topk_predictions.csv\"\n",
        "    }\n",
        "    results_df = evaluator.evaluate_csvs(files)\n",
        "    display(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "zP1ZYgL0Xg8N",
        "outputId": "068db39f-007e-4448-f454-e1276d9cc902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "            Version  ROUGE-1  ROUGE-2  ROUGE-L    BLEU  BERTScore  \\\n",
              "0   custom_topk_val   0.0473   0.0011   0.0417  0.0013     0.7776   \n",
              "1  custom_topk_text   0.0465   0.0011   0.0405  0.0014     0.7785   \n",
              "\n",
              "   Avg Token Repetition  \n",
              "0                0.6539  \n",
              "1                0.6535  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-19979d65-55f6-42d4-8535-cf3613c082c1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Version</th>\n",
              "      <th>ROUGE-1</th>\n",
              "      <th>ROUGE-2</th>\n",
              "      <th>ROUGE-L</th>\n",
              "      <th>BLEU</th>\n",
              "      <th>BERTScore</th>\n",
              "      <th>Avg Token Repetition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>custom_topk_val</td>\n",
              "      <td>0.0473</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0417</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.7776</td>\n",
              "      <td>0.6539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>custom_topk_text</td>\n",
              "      <td>0.0465</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0405</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.7785</td>\n",
              "      <td>0.6535</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-19979d65-55f6-42d4-8535-cf3613c082c1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-19979d65-55f6-42d4-8535-cf3613c082c1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-19979d65-55f6-42d4-8535-cf3613c082c1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-849818e6-35bf-4823-a96d-08d8d977fac5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-849818e6-35bf-4823-a96d-08d8d977fac5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-849818e6-35bf-4823-a96d-08d8d977fac5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_e8277602-dd43-432b-91ab-9c759763b4fa\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_e8277602-dd43-432b-91ab-9c759763b4fa button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Version\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"custom_topk_text\",\n          \"custom_topk_val\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ROUGE-1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0005656854249492395,\n        \"min\": 0.0465,\n        \"max\": 0.0473,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0465,\n          0.0473\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ROUGE-2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0011,\n        \"max\": 0.0011,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0011\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ROUGE-L\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0008485281374238569,\n        \"min\": 0.0405,\n        \"max\": 0.0417,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0405\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BLEU\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.07106781186548e-05,\n        \"min\": 0.0013,\n        \"max\": 0.0014,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0014\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BERTScore\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0006363961030679012,\n        \"min\": 0.7776,\n        \"max\": 0.7785,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.7785\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Avg Token Repetition\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00028284271247466634,\n        \"min\": 0.6535,\n        \"max\": 0.6539,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.6535\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}