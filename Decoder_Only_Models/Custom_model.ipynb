{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5540839-6495-46f7-82c2-118e0ae8393b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ra_tanvi/.conda/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eca9fb6f-71c3-445c-9abc-6836e17d161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Load Dataset\n",
    "dataset = load_dataset(\"google/code_x_glue_ct_code_to_text\", \"java\")\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"validation\"]\n",
    "train_data = train_data.select(random.sample(range(len(train_data)), int(len(train_data) * 0.3)))\n",
    "val_data = val_data.select(random.sample(range(len(val_data)), int(len(val_data) * 0.3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c57e6c33-d07b-4e65-babd-67da88314c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# âœ… Tokenizer setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>', 'sep_token': '<sep>', 'bos_token': '<s>', 'eos_token': '</s>'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1689b367-e18e-440b-978f-a1b83c1bcc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49476/49476 [00:22<00:00, 2186.63 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1554/1554 [00:00<00:00, 2390.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# âœ… Tokenization\n",
    "def tokenize(example):\n",
    "    code = example[\"code\"]\n",
    "    docstring = example[\"docstring\"]\n",
    "    input_text = f\"<s> {code} <sep> {docstring} </s>\"  # ðŸ‘ˆ Still used for full tokenization\n",
    "    tokenized = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "    # Create labels that mask code tokens, only docstring is target\n",
    "    try:\n",
    "        sep_idx = tokenized[\"input_ids\"].index(tokenizer.convert_tokens_to_ids(\"<sep>\"))\n",
    "    except ValueError:\n",
    "        sep_idx = 128  # Fallback if separator is not found\n",
    "    \n",
    "    # Only predict tokens AFTER the <sep>\n",
    "    labels = [-100] * (sep_idx + 1) + tokenized[\"input_ids\"][sep_idx + 1:]\n",
    "    labels += [-100] * (256 - len(labels))\n",
    "    tokenized[\"labels\"] = labels[:256]\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "train_data = train_data.map(tokenize)\n",
    "val_data = val_data.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6363cb2-ef03-4d92-91c3-0525e312e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Dataset Class\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.input_ids = torch.tensor(hf_dataset[\"input_ids\"])\n",
    "        self.labels = torch.tensor(hf_dataset[\"labels\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.input_ids[idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = CodeDataset(train_data)\n",
    "val_dataset = CodeDataset(val_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71f6317d-b576-4f10-843d-ca4833251678",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# âœ… Decoder-Only Model\n",
    "class DecoderOnlyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, num_layers, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=n_heads)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        x = self.embedding(x) + self.pos_embedding(positions)\n",
    "\n",
    "        tgt_mask = torch.triu(torch.ones(seq_length, seq_length, device=x.device), diagonal=1)\n",
    "        tgt_mask = tgt_mask.masked_fill(tgt_mask == 1, float('-inf'))\n",
    "\n",
    "        memory = torch.zeros((seq_length, batch_size, x.size(-1)), device=x.device)\n",
    "        x = self.transformer_decoder(x.permute(1, 0, 2), memory, tgt_mask=tgt_mask)\n",
    "        return self.fc_out(x.permute(1, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d121c4-c15b-4a5f-90f4-7d288e3ed039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training M8 with d_model=768, layers=6, heads=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "M8 Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6185/6185 [09:59<00:00, 10.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.8673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "M8 Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6185/6185 [10:54<00:00,  9.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.9518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "M8 Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6185/6185 [12:18<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.5918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "M8 Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6185/6185 [16:44<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.3925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "M8 Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6185/6185 [16:49<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.2844\n",
      "M8 Final Validation Perplexity: 1.28\n",
      "Training M10 with d_model=768, layers=8, heads=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "M10 Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6185/6185 [19:48<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.9471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "M10 Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6185/6185 [19:48<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.2511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "M10 Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6185/6185 [19:49<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.0373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "M10 Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6185/6185 [19:49<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.8972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "M10 Epoch 5:   9%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                                        | 570/6185 [01:51<18:25,  5.08it/s]"
     ]
    }
   ],
   "source": [
    "# âœ… Train Function\n",
    "def train_model(name, d_model, n_layers, n_heads, epochs=5):\n",
    "    print(f\"Training {name} with d_model={d_model}, layers={n_layers}, heads={n_heads}\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = DecoderOnlyModel(len(tokenizer), d_model=d_model, n_heads=n_heads, num_layers=n_layers).to(device)\n",
    "    model.embedding.weight.data.normal_(mean=0.0, std=0.02)\n",
    "    model.fc_out.bias.data.zero_()\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"{name} Epoch {epoch+1}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Save model + check perplexity\n",
    "    torch.save(model.state_dict(), f\"{name}_final_model.pth\")\n",
    "    ppl = calculate_perplexity(model, val_loader, criterion, device)\n",
    "    print(f\"{name} Final Validation Perplexity: {ppl:.2f}\")\n",
    "\n",
    "train_model(\"M8\", d_model=768, n_layers=6, n_heads=8, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "791e92b4-e36e-4e6c-9378-6538389b7cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': '<pad>', 'sep_token': '<sep>', 'bos_token': '<s>', 'eos_token': '</s>'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a722645-be88-42ee-9091-7eb4e1b1ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DecoderOnlyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, num_layers, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=n_heads)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        x = self.embedding(x) + self.pos_embedding(positions)\n",
    "        tgt_mask = torch.triu(torch.ones(seq_length, seq_length, device=x.device), diagonal=1)\n",
    "        tgt_mask = tgt_mask.masked_fill(tgt_mask == 1, float('-inf'))\n",
    "        memory = torch.zeros((seq_length, batch_size, x.size(-1)), device=x.device)\n",
    "        x = self.transformer_decoder(x.permute(1, 0, 2), memory, tgt_mask=tgt_mask)\n",
    "        return self.fc_out(x.permute(1, 0, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e63bf5ba-42d5-4470-b1e9-a9b367da48f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderOnlyModel(\n",
       "  (embedding): Embedding(50261, 768)\n",
       "  (pos_embedding): Embedding(256, 768)\n",
       "  (transformer_decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=768, out_features=50261, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Matching architecture\n",
    "d_model = 768\n",
    "# n_layers = 8 if model_name == \"M10\" else 6\n",
    "n_heads = 12 \n",
    "\n",
    "# Initialize and load weights\n",
    "model = DecoderOnlyModel(len(tokenizer), d_model, n_heads, n_layers)\n",
    "model.load_state_dict(torch.load(f\"{model_name}_final_model.pth\", map_location=\"cpu\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5ef0338-48ee-4c4a-aadd-d2042073d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_docstring(model, tokenizer, code_snippet, max_length=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    input_text = f\"<s> {code_snippet} <sep>\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        outputs = model(input_ids)\n",
    "        next_token_logits = outputs[:, -1, :]\n",
    "        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    output_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return output_text.split(\"<sep>\")[-1].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a07668e5-5e6c-404c-b776-9b749ae3be06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated docstring: public int add(int a, int b) { return a + b; } s Create theience route Bind Return file elementven Create attribute map - Create bound.. in in path path path left option controller proxy rate rate video users users users users users users users users users users users users users users users users users users users usersatteratterDeviceDeviceDeviceWebitionutiodvenuevenuevenuevenuevenue Campaign budget budgetreshshotshotshotshotshotshotshotshotshotshotshotshotshotshotshotshotshotshotshotshotshotshotshotshotshotshotshotshotshotshotshotshot\n"
     ]
    }
   ],
   "source": [
    "# Testing on model M8\n",
    "java_code = \"public int add(int a, int b) { return a + b; }\"\n",
    "docstring = generate_docstring(model, tokenizer, java_code)\n",
    "print(\"Generated docstring:\", docstring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a316e-499b-447c-8bcd-b6ce16777225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
