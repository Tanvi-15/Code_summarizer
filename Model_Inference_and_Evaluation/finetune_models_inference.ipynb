{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c6af94f0",
      "metadata": {
        "id": "c6af94f0"
      },
      "source": [
        "# Inference and Evaluation of Fine-tuned CodeT5 for Java Code Summarization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8i-bWJ8IaR2h",
      "metadata": {
        "id": "8i-bWJ8IaR2h"
      },
      "source": [
        "**INSTALL LIBRARIES**\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "71856a92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71856a92",
        "outputId": "9f75aacd-0035-4097-bce3-8079fabe03ed"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install transformers datasets evaluate rouge_score bert_score --quiet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1pSzJiqukqf",
      "metadata": {
        "id": "a1pSzJiqukqf"
      },
      "source": [
        "**LOAD HF DATASET**\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "857558e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589,
          "referenced_widgets": [
            "e98148252d644926bce823aea33c2e9d",
            "e94bbfcf4dcb4ce6b56412c0a2ed39a9",
            "63e97479b1e24f46b3e6e5cef0bdffbe",
            "0b8b2a7f79f7435e97ca0710892c178c",
            "22d58ea2cbb340e5a1bbb76615b428fa",
            "6b1d1ad67d5f4712bb4e3a146e8a0057",
            "894576b2111543198ab291e2e47ad9c5",
            "6c382cf5e4904ded875ba8f104e40d8d",
            "acc723a60b1f414aa090a46e0a65b303",
            "e5dc6fedd0844836a096abb93c386e54",
            "5f4eb69d0bdd4ab0b2ac705aec4df798",
            "3e01216f8df54df1aff115de85259f7b",
            "2c335dfdc20a4c5a8462484974006371",
            "724ccbd37f314ff78db4458529ab2d90",
            "c3b0588e923d4b74949f99869fa1da50",
            "acea4555e4ff4fc692d8817a4f8a9363",
            "5e32c63c4fc94b3f95b004f9c324eb17",
            "9f6eef6485ab48fba3abdc88dffe2ed8",
            "08f2df129ee6422c897c5b65700d80a1",
            "1fce81814481463681dc15d8b003decf",
            "dcfd6815bcc044988f513504f66a4b20",
            "91ec3cffd75642fe9b4028a65ca5a8e7",
            "f9be873b19584e05bb53e55c593b27b9",
            "33f8e76e39754d41b258e5a58a3b4b0d",
            "e798c335460741fca2a3a729837f2b11",
            "f6e0d8a770444e25b54283f29210185d",
            "6d893df346cb449ab1c5ff93a005d9b9",
            "3bd9ec5870724348a60545ac98bf4db2",
            "d5e7cc0fdde942eba17fee6845cd91d4",
            "5e09b684811d4f0eac1af3c7b76aeaa0",
            "e6826abfd7c743e695d44624a67ba509",
            "9e6b85a5d16e4efe8b25146ba4e65a62",
            "c583bb0d19964ca995564fc1b444b2fb",
            "1630f7c444fb4be4a3d6b94886470ae4",
            "e9e3f2ce505b4ea1a962888c60abd9a3",
            "7ead6c03a8af44ae80b01a3d1f435645",
            "6665b498987c4b919019419eafce735b",
            "ded03bcb9b844e3f80519b99fa379344",
            "b63369a5a3574fb7beec3b1a3d3e067c",
            "df318c90a4404f848ae0a11e788de7fe",
            "5c60c9d0a8704a889524a814f8c4c547",
            "f1f6c288b25c4b00aeb6ac44f8b7db12",
            "7cd06d6badbc488193c59f40b624495a",
            "33ee4af576e0479094c0f43832884b04",
            "614a2cbd4b7e427ab383e7f5319d7c55",
            "4543ccc71a7f4d108a153ed836b51219",
            "e1182c8dacd54e09aa1125e247b1a60d",
            "e3e24dac13f546ad8bf08af0db17bbf7",
            "d17778f8a6e74c1b854ae26616be4824",
            "ccb8e3d35cd444fbbcb368606158fb2c",
            "a280b52d19b8443bb22194957772b543",
            "02d25beb4c694d6a9e847e9ed4c5c86e",
            "3ed0c5e8915945ad84753cf9980d6f34",
            "18f373ebb0324cde936733f4290b38dd",
            "81b0d6ae3d1946d09febbfd71acca5e1",
            "d2fc800b22454c73bd3ff52b6f3248af",
            "d023ecf7ee5c42678c8d990050be337d",
            "d58a3560813c42b5be4c63c36e23fb25",
            "941222afd42d4465858ea88ea1c1745d",
            "413864b52179425683920b608fc60851",
            "c5ee48b6eb7145fcbae78c2b8fae8a55",
            "9f915d210aa04a60b4e56755de852e19",
            "d4545df200084566bbfa7728d719ac0e",
            "54c81b9f714e4722ae1c0f5d7c779af2",
            "a204da912d5b4fbb88b3a5be9f549097",
            "eb462af1ff8148629fc572c54a1567f4",
            "151bcc8fff87464cad5c4330e851a838",
            "e5e23d622bb54f40800ce355094fa970",
            "62d2feed151c470da8aa29abb3c37367",
            "5ecfc12fea474e98b7be6c4ff0635f50",
            "39c2c9aefa5c427e8fa9947e50689437",
            "0c6643706b2c4ce981584cdc53a08091",
            "ae9c5aa33c1241ba88cd23c48058e117",
            "dd9a7e5ed5134f03829a0a7ef7282f06",
            "5b4e3fae8a394617b2a07db793783a4d",
            "8cb7951dda5341fd87b3e22a356ff03c",
            "9c0d1fbfd890488aaf804e0063081b52",
            "567bde4d5d1347799b81da98b5bdc8db",
            "b94e0052081441dbbcf3fbdb9b1ca647",
            "c0c9a8de042749eab24396b124be9330",
            "3fe920a2971444b8a6a28c2463602bfa",
            "5d169a85aa9443d8b52a8b8cfb28ceb0",
            "043e188597354d20912d6934d8f2c9c9",
            "cf04b4471eb94787a737a25b353698f9",
            "94553774544646bdb6b0adf924187259",
            "29bfd50280c945aa868c8e52a59c0ed4",
            "1b73033a05f64a04a6c6784be2d39cc6",
            "1d3c88bf6fbd49f584877ebaf82c2ca3",
            "f53220ee18e640e2b2cb2fc10a2c2145",
            "22adb11aad75418c997ba7bdec85e497",
            "d8a9228ddd22419e8cecf39ed5591a82",
            "ea07446b1b2e467288210e19ce438f81",
            "52bbdf935142491b91096163af30cd88",
            "5a30197fdbc44f64b5aec74d21811f05",
            "0fc3a393f9b8442da8d174ef4454e1de",
            "3a2b98684d57491ab267fbb9528c25cf",
            "b7a2489a13be4e68b7abdd17159d8bc3",
            "78a24ea89a16437c825b1421f3a009ea",
            "67e9ecefc1a04f919364e0a01d9edc2a",
            "634d8da2e0a243f8912f488e4398d95f",
            "b719529180b54832a555a3f4a4f177e5",
            "cae6ace2ca274dd49be3bcc12b7e0d07",
            "374eb5fb3a614a2f8d335c1ecc06c06d",
            "5ef176060ddd49b6919158506d7efb50",
            "6bb52d9aa2b342cd87fbdc4c7328d742",
            "219a8faf960f410d8726b2e4145d1f9a",
            "facebab6eb8d48acba571e00eb4daae3",
            "347177699a644779a93adeb28126e759",
            "e1478ef9d9934a30b109f97a4b32c232",
            "87aea0a9d5dd4a85bd3fd61cc3673015",
            "3b4112dda21345d48940589850104b78",
            "e204d3373eef4c9c9beac0078d087910",
            "1e87a24089314e20b5a040e969b6b7ae",
            "e4d57056e97f48de91cbe677b4c0e47b",
            "05af7d50e30c44d3bc332c26a9fbdbb2",
            "7ac8cf1d1a2546508c6b9247f7839c2e",
            "894fb6e8455743c59061711dbd1f4c6e",
            "6d83850027a645e092277b123c7f19eb",
            "4ee1d1e212a44a6cade2a83041e9b1a3",
            "7b3389dd015c4130900a009c1771f7f3",
            "c57b437606164627953c5bcd0a12f0d9",
            "c3be70d7a28440fcac574c0655ca0d6f",
            "af65500b9d35461ba1e31a7b17ed8e39",
            "363a973cc2a14560b56e7f732fbcb871",
            "e73263d3e14142ef86472a3903335796",
            "e8e8d188689b4642ad9cacd7e72e68d3",
            "441cceec7ad34c64bd750aa2876af5ab",
            "6601adaf542e4d77b952206a05d57b5f",
            "d3fff95291944470b65d72e119367ea6",
            "c287b094c91444dc948fba74026270fd",
            "0d08ca48209748459b935ab14a237e60",
            "fe25a6243b2b446eb55f8e90b62b0d5b",
            "524cbc01cb7f449cba91be5de684a0cc",
            "e0fa917b5c6a417badd4719b2484f8d3",
            "e54777009b9c49b5b290c46670670dbd",
            "4b1610e8df52458c9cb885094b0f778a",
            "5b85ddf630024fd59807f219b79d9e9c",
            "349858ab0dbb4238b3807367b01b0c19",
            "fd792fcc949d4b27ba78d8d66e9f2da8",
            "94c11d31d76c4972b4bb4771d53c739a",
            "0233418cf0fe497096a2fc8c97853949",
            "f614df0b22714ec3891becec24925db2",
            "56b91a18f9684f6b9a3fefe598d209b9",
            "ca9372dab2904c7b930c215e38847ee3",
            "5de241c85749414a9167394633b92a3b",
            "435254fb8a634949a7ca1b06cbad26d7",
            "a27eeb74406746a4b685230ac39ba1ab",
            "2f3d35a603f34e4ba7e84500a0e043bc",
            "75fe9a6105b04adab187629c9b18640a",
            "8e9e44df82254494a078fac2d9981775",
            "f006ac6011ab4b58882f9c839c062323",
            "a1ae4c452d164259b070a2162cca3926",
            "82087eb2a1f04403a132a4c3208d6742",
            "1bc273e520734943850f5a0bcb71fbcf"
          ]
        },
        "collapsed": true,
        "id": "857558e9",
        "outputId": "f522d3b1-1d9b-455e-b6a2-b6d37d832b31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Tanvi\\miniconda3\\envs\\nlpProject\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"java\")\n",
        "# model = GPT2LMHeadModel.from_pretrained(\"./gpt2-docstring-model\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-docstring-model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "--uUolcQu_sn",
      "metadata": {
        "id": "--uUolcQu_sn"
      },
      "source": [
        "**INFERENCE**\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "pnWn3nTYt3-a",
      "metadata": {
        "id": "pnWn3nTYt3-a"
      },
      "outputs": [],
      "source": [
        "# CodeSummaryGenerator\n",
        "# This class handles preprocessing, batch inference, and single-example generation for code summarization\n",
        "# using a fine-tuned CodeT5 model from Hugging Face.\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import List, Dict, Optional\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "\n",
        "class CodeSummaryGenerator:\n",
        "    def __init__(self, model_path: str, decoding_config: Dict, device: Optional[str] = None):\n",
        "        # Load tokenizer and model from Hugging Face\n",
        "        self.model_path = model_path\n",
        "        self.decoding_config = decoding_config\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        if model_path.startswith(\"pritammane105/GPT2-Code-Summarisation\"):\n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "            self.model = GPT2LMHeadModel.from_pretrained(model_path).to(self.device)\n",
        "        else:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(self.device)\n",
        "        # self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        # self.model = AutoModelForSeq2SeqLM.from_pretrained(model_path).to(self.device)\n",
        "        # self.tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "        # self.model =GPT2LMHeadModel.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.eval()\n",
        "\n",
        "    def preprocess_dataset(self, split: str = \"validation\", max_input_length: int = 512):\n",
        "        # Load and tokenize dataset split (validation or test)\n",
        "        dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"java\")[split]\n",
        "\n",
        "        def tokenize_fn(example):\n",
        "            return self.tokenizer(\n",
        "                example[\"code\"],\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=max_input_length,\n",
        "            )\n",
        "\n",
        "        tokenized = dataset.map(tokenize_fn, batched=True)\n",
        "        tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "        return tokenized, dataset[\"docstring\"]\n",
        "\n",
        "    def generate_summaries(self,\n",
        "                            tokenized_data,\n",
        "                            references: List[str],\n",
        "                            save_path: str,\n",
        "                            batch_size: int = 16,\n",
        "                            save_every: int = 50):\n",
        "        # Perform batch inference on the dataset and periodically save output CSV\n",
        "\n",
        "        val_loader = DataLoader(tokenized_data, batch_size=batch_size)\n",
        "        generated_summaries = []\n",
        "        start_batch = 0\n",
        "\n",
        "        # Resume if already saved the file with partial results\n",
        "        if os.path.exists(save_path):\n",
        "            df_existing = pd.read_csv(save_path)\n",
        "            generated_summaries = df_existing[\"predicted_summary\"].astype(str).tolist()\n",
        "            start_batch = len(generated_summaries) // batch_size\n",
        "            print(f\"⏩ Resuming from batch {start_batch} (already {len(generated_summaries)} predictions)\")\n",
        "\n",
        "        for i, batch in enumerate(tqdm(val_loader, desc=\"Generating Summaries\")):\n",
        "            if i < start_batch:\n",
        "                continue\n",
        "\n",
        "            input_ids = batch[\"input_ids\"].to(self.device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    **self.decoding_config\n",
        "                )\n",
        "\n",
        "            decoded = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "            generated_summaries.extend(decoded)\n",
        "\n",
        "            # Periodically save results to disk\n",
        "            if (i + 1) % save_every == 0 or (i + 1) == len(val_loader):\n",
        "                print(f\"💾 Saving at batch {i + 1}\")\n",
        "                df = pd.DataFrame({\n",
        "                    \"gold_summary\": references[:len(generated_summaries)],\n",
        "                    \"predicted_summary\": generated_summaries\n",
        "                })\n",
        "                df.to_csv(save_path, index=False)\n",
        "\n",
        "    def generate_single(self, code_snippet: str):\n",
        "        # Generate a summary for a single code snippet\n",
        "        inputs = self.tokenizer(\n",
        "            code_snippet,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.model.generate(**inputs, **self.decoding_config)\n",
        "\n",
        "        return self.tokenizer.decode(output[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xNDjiHJbvExS",
      "metadata": {
        "id": "xNDjiHJbvExS"
      },
      "source": [
        "**EVALUATION**\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "-cxnNWI8_YrV",
      "metadata": {
        "id": "-cxnNWI8_YrV"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class SummaryEvaluator:\n",
        "    def __init__(self):\n",
        "        self.rouge = evaluate.load(\"rouge\")\n",
        "        self.bleu = evaluate.load(\"bleu\")\n",
        "        self.bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "    def avg_token_repetition(self, predictions):\n",
        "        \"\"\"\n",
        "        Computes the average number of repeated tokens per prediction.\n",
        "        A high repetition score indicates redundancy in the generated text.\n",
        "        \"\"\"\n",
        "        rep_counts = []\n",
        "        for text in predictions:\n",
        "            tokens = text.strip().split()\n",
        "            counts = Counter(tokens)\n",
        "            repeated_tokens = sum(v for v in counts.values() if v > 1)\n",
        "            rep_counts.append(repeated_tokens / max(1, len(tokens)))\n",
        "        return np.mean(rep_counts)\n",
        "\n",
        "    def evaluate_csvs(self, files: Dict[str, str]):\n",
        "        \"\"\"\n",
        "        Evaluates multiple prediction files and returns a DataFrame of metrics.\n",
        "        Each file must be a CSV with columns: 'predicted_summary' and 'gold_summary'.\n",
        "        \"\"\"\n",
        "        all_results = []\n",
        "        for name, path in files.items():\n",
        "            if not os.path.exists(path):\n",
        "                print(f\"File not found: {path}\")\n",
        "                continue\n",
        "\n",
        "            df = pd.read_csv(path)\n",
        "            predictions = df[\"predicted_summary\"].astype(str).tolist()\n",
        "            references = df[\"gold_summary\"].astype(str).tolist()\n",
        "\n",
        "            rouge_scores = self.rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
        "            bleu_score = self.bleu.compute(predictions=predictions, references=references)\n",
        "            bert_score = self.bertscore.compute(predictions=predictions, references=references, lang=\"en\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            repetition = self.avg_token_repetition(predictions)\n",
        "\n",
        "            all_results.append({\n",
        "                \"Version\": name,\n",
        "                \"ROUGE-1\": round(rouge_scores[\"rouge1\"], 4),\n",
        "                \"ROUGE-2\": round(rouge_scores[\"rouge2\"], 4),\n",
        "                \"ROUGE-L\": round(rouge_scores[\"rougeL\"], 4),\n",
        "                \"BLEU\": round(bleu_score[\"bleu\"], 4),\n",
        "                \"BERTScore\": round(np.mean(bert_score[\"f1\"]), 4),\n",
        "                \"Avg Token Repetition\": round(repetition, 4)\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(all_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6olT2SCH5mks",
      "metadata": {
        "id": "6olT2SCH5mks"
      },
      "source": [
        "**INFERENCE & EVALUATION**\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-7LObn20_gRB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 876,
          "referenced_widgets": [
            "9dca71a1bd564a91bb0906678cae2363",
            "7d885a3efe2b49ad8403b62eb3765586",
            "5921567c78a84d5f85f598f576db278e",
            "d7c128b88cf149119b17799afc7722de",
            "87d243bb6b924136a6be62e748b81549",
            "4f4f3d5d15fe4bf9bf4299c49ae717cc",
            "6c1f5132182943f3816183cf04780734",
            "41491cad62284741946be0dd21bf4e06",
            "10102a60793948c4ad3adbc00de98164",
            "dd6a7fb515c34f86b8d7549ae7dd5bb3",
            "cf86445d52a24b8e8487ed659dce1ba7",
            "85fd8c67079d4608a1b96278b422b401",
            "5b505762f31b422b8d2fca70d6213067",
            "5c1c98d563da4c90a9036759d5841cbb",
            "e0fe4225a72d4c6ebe50ba12f8c0b444",
            "dff4bc5c6a7d42d09be95781a5b6925c",
            "06cd901bec2b427391d04449f5154107",
            "37a4865c2d0e4a2fbcd32bb22da80a63",
            "2c4bf3a8f7b44404be89a8a996545f9d",
            "8db5daf4d59347af9ed47b6c904f9830",
            "776d51beb5eb46128a0b1b4ce24ec306",
            "844645dc2f3b4da4bec5ee2859f2cd4d",
            "be478aaeec804457942ed4ac41076310",
            "ec0a994978184ea6a0634c67291b3832",
            "b7e80d1dd347499f866550b034e2b81b",
            "0b10349c72a2461b9dbc9a3dbb734b26",
            "84fe3cf3b6f14ea1a31589961f560ae0",
            "a2a4715426e1450c96e2323e033b14d6",
            "fc70af69ed174c9f88d02370a8cf2715",
            "e653b5a193824f369f74807b4d969998",
            "5483a7fed1d24b06834b33e4222629f4",
            "e1a080402bea45559f58b1a35ea2bb0e",
            "8121c8396b2c4130b2f4c3a50c84a76b",
            "28c3b7b55dc24494b66c4a76a2720bf2",
            "5d87871a07944f9893ea9d65dfe5face",
            "c00036af0d884b42959876191cfd8b1f",
            "7b0ca6ae6c4049b98f02dc90484f2fc8",
            "0809e52166df481abe161a9a3e453d03",
            "e918bc48a51c496d9332a514dccf8333",
            "031e8b021c5e4c11ad62f3e4f30e72b3",
            "c3caba4278af46c98bbf3d48da9220cc",
            "5fef3029d8bf440f98cedaa38c4f6802",
            "ef2e6d5982114ef7b9f967fb8671c064",
            "94606127963e48bba47313b164da3ba7",
            "d9318ea67b064b289bc94fd4f47052ac",
            "7f736e3eb1f646b296c4b01dc5817d35",
            "a6092fe5914b4655a133436632a183cf",
            "3385c606103e4018879efab00b96ca25",
            "48d9416414d74196862901b063367b2f",
            "a0539f92c217470982dbd38e3e9bcd15",
            "6248af58a07c445bbe8d5354e1c8027e",
            "e6389b519d0b4f41906ab2f711d20a9a",
            "12df1dc521c5401d87f6e6e75512ec0b",
            "838558cabeca4ee5a170e34d4aca4f1e",
            "3a2b159c5a5a4133bc7cb26fae580a15",
            "dbf3603b213045058a1d836fcb8f666c",
            "c038d1db9fdf4ded89443922697cf91a",
            "87825133eae8446eb651d723cd12d586",
            "9a5c29beda3e4c6aa0d420ccf1ca366a",
            "a5ea0f59b2624fad8964e1612051b6c9",
            "705d167dd96d4f6fb18a851e74709076",
            "fc9cb64dcccc48c19a1043d43facc662",
            "e69cddd76993416382e464aa70b5b055",
            "93d8a6cc63754b068ff4266e80811e3a",
            "916461ee620f4671846619b4acf405d3",
            "68c14fb65c1b48be95d2986ce6ea72e1",
            "b28a485c2973480b970b6b7dfbe0bdaa",
            "f91d8f9b6dff4811a1fd330834d0dde8",
            "095bbe010ee44b1da87c31e86d20db1e",
            "5abf7dbea82042dd9426248aa01ae50d",
            "1dfef7c90ef74422979a88868d6eb86d",
            "407b889dd613498095a6d202950cbf01",
            "cea1ee6f905b47679908317075cce070",
            "d8f6bbc2076546938d04730f2845741a",
            "c25effc56e344c228db0a1e1bce87aac",
            "30610e8217204409bea68cc63287b259",
            "71a95681c63144a9b97f075531252b6b",
            "a8f007ead38942799f777323b8efa0fc",
            "f765e6f3df274f42b448335fffe8ffcb",
            "770ca1f1de6547ffbe64f98811531f34",
            "129f118217644893a6bf1c6828697f40",
            "b2f6a66feea04d659746a571bcbf1466",
            "53c3eac774d0441a97b723530d3a3818",
            "942233c69f60430bb41423b9baa69832",
            "1d496bbf86dc4c919750668a7e02512f",
            "04085fd28d8849e58ccdc17b05e7cb0a",
            "705974719cd9497f9ecceb9b3e7199d9",
            "32a700c2b3a44964b9b97667470deb4b",
            "8d03737131c44dba858bc858e7517a80",
            "3fd2cc476c204dcbb998ea5d67012a26",
            "bae230e9957e45cd9bfcad82caa99d8e",
            "33bbaf317b954042b25ed70744c98f64",
            "764e126e8ca64dc9a0c0872eb19be448",
            "6f1c2ac0b54842049b12783b4e10ca7a",
            "8fbfe9c2dc0e4bc88a3eda33f324e23b",
            "b2169a278b1a4ab482a3f90db1e8d51d",
            "a54a05373f9748acbac898ef5cdc6d71",
            "7dccec01ff444522966d9c6e5902fd6b",
            "0601de07072d4924becf36a5084657cf",
            "8943e9819df4473e8f01131f900bae99",
            "bbe7ee528a304f8ca1a368483558878c",
            "de94e6dfb70b4f57aa18fa125064af83",
            "ef2aee7227104f04b357c8c13bc440e3",
            "3ffc0fc1270d4ba39913b445b8446965",
            "075c33cf46744956883cd25f8852ec44",
            "d22ffbc49c3b4af4a892913f0b24f0e5",
            "a72c4fa5221b4e99a85ec42f2eb4b39c",
            "232b6086f1634e3ab975f1a87aea9ccf",
            "5d9c77ae0f4542a3a73e6c7952e5e6e0",
            "1ad0ad82b2d44059878d81de902d43f7",
            "d5093bf4dcaf4546b423ab4cb4095a60",
            "9f98177ff2a24a3d90ce210f9de596a0",
            "e42e59361bc4400a87b46ac8bf6bb412",
            "0008f1ca542f4e5f89904241a90d8509",
            "00155edb938d404b9eca979bea12d555",
            "b05b73a513e2479ca30fad2041fc14b5",
            "b41e12758ab64be48f684e2c5f20e0fc",
            "68dc232b1fda4e51856096242e6d21b0",
            "d009a2dade1d4126a917344d57e354be",
            "15901af6f14c4803b9572288e200cc6e",
            "5385a7e372944db69d5a257b06ca8387"
          ]
        },
        "id": "-7LObn20_gRB",
        "outputId": "6f005747-afcc-4d62-bddf-ee384393a3e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Running config: baseline_beam\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 5183/5183 [00:19<00:00, 260.61 examples/s]\n",
            "Generating Summaries:   0%|          | 0/324 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "# Inference and Evaluation Pipeline for CodeT5\n",
        "# This script runs multiple decoding strategies on the validation and test sets\n",
        "# and generates a metrics comparison table with ROUGE, BLEU, BERTScore, Exact Match, and Repetition analysis.\n",
        "\n",
        "# from inference_pipeline import CodeSummaryGenerator\n",
        "# from eval_pipeline import SummaryEvaluator\n",
        "\n",
        "\n",
        "def runConfigs(model_path):\n",
        "    # Define decoding strategies\n",
        "    decoding_configs = {\n",
        "        \"baseline_beam\": {\n",
        "            \"max_new_tokens\": 64,\n",
        "            \"num_beams\": 4,\n",
        "            \"early_stopping\": True\n",
        "        },\n",
        "        \"beam_repetition\": {\n",
        "            \"max_new_tokens\": 64,\n",
        "            \"num_beams\": 4,\n",
        "            \"early_stopping\": True,\n",
        "            \"repetition_penalty\": 1.3\n",
        "        },\n",
        "        \"sampling_topk\": {\n",
        "            \"max_new_tokens\": 64,\n",
        "            \"do_sample\": True,\n",
        "            \"top_k\": 50,\n",
        "            \"temperature\": 0.7,\n",
        "            \"repetition_penalty\": 1.2\n",
        "        }\n",
        "    }\n",
        "    # Output files (local/Colab environment)\n",
        "    if model_path==\"pritammane105/GPT2-Code-Summarisation\":\n",
        "        output_files = {\n",
        "            \"baseline_beam\": \"GPT2_val_predictions.csv\",\n",
        "            \"beam_repetition\": \"GPT2_val_beam_repetition.csv\",\n",
        "            \"sampling_topk\": \"GPT2_val_sampling_topk.csv\",\n",
        "            \"test_sampling_final\": \"GPT2_test_sampling_output.csv\"\n",
        "        }\n",
        "    else:\n",
        "        output_files = {\n",
        "            \"baseline_beam\": \"CodeT5_val_predictions.csv\",\n",
        "            \"beam_repetition\": \"CodeT5_val_beam_repetition.csv\",\n",
        "            \"sampling_topk\": \"CodeT5_val_sampling_topk.csv\",\n",
        "            \"test_sampling_final\": \"CodeT5_test_sampling_output.csv\"\n",
        "        }\n",
        "\n",
        "    # Run decoding strategies for validation set\n",
        "    for name, config in decoding_configs.items():\n",
        "        print(f\"\\n Running config: {name}\")\n",
        "        generator = CodeSummaryGenerator(model_path, config)\n",
        "        val_tokenized, val_refs = generator.preprocess_dataset(\"validation\")\n",
        "        generator.generate_summaries(val_tokenized, val_refs, output_files[name])\n",
        "\n",
        "    print(\"\\n Evaluating generated summaries...\")\n",
        "    evaluator = SummaryEvaluator()\n",
        "    results_df = evaluator.evaluate_csvs(output_files)\n",
        "    display(results_df)\n",
        "\n",
        "\n",
        "runConfigs(\"pritammane105/GPT2-Code-Summarisation\")\n",
        "runConfigs(\"pritammane105/CodeT5-Java-Summarisation\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2441fee8",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "970dc9a3",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47a0e9e1",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5df4aa48",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1428dba",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9qTDKnX82HIf",
      "metadata": {
        "id": "9qTDKnX82HIf"
      },
      "source": [
        "**Miscellaneous Test Samples**\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------\n",
        "----------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f2618f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f2618f2",
        "outputId": "13c22e71-1cdc-4a65-ff3a-3ef8ad329f65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Add two integers.\n",
            "\n",
            "@param a first integer\n",
            "@param b second integer\n",
            "@return the sum of the two integers\n"
          ]
        }
      ],
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "# # Load fine-tuned model from Google Drive\n",
        "# model_dir = \"/content/drive/MyDrive/codet5_checkpoints/checkpoint-46386\"\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
        "\n",
        "# def generate_summary(code_snippet):\n",
        "#     inputs = tokenizer(code_snippet, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
        "#     outputs = model.generate(**inputs, max_length=128, num_beams=5, early_stopping=True)\n",
        "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# java_code = \"\"\"\n",
        "# public int add(int a, int b) {\n",
        "#     return a + b;\n",
        "# }\n",
        "# \"\"\"\n",
        "# print(generate_summary(java_code))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E8vcgSfrgV2J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8vcgSfrgV2J",
        "outputId": "28092da9-164d-4b31-ee4c-c1165e1172dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computes the difference between two integers.\n",
            "\n",
            "@param a first integer\n",
            "@param b second integer\n",
            "@return the difference\n"
          ]
        }
      ],
      "source": [
        "java_code = \"\"\"\n",
        "public int perform(int a, int b) {\n",
        "    return a - b;\n",
        "}\n",
        "\"\"\"\n",
        "print(generate_summary(java_code))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dyeQ6nWjgVtT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyeQ6nWjgVtT",
        "outputId": "3c23e466-a76c-4d88-d72a-9536afe14752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Add two integers.\n",
            "\n",
            "@param a the first integer\n",
            "@param b the second integer\n",
            "@return the result\n"
          ]
        }
      ],
      "source": [
        "java_code = \"\"\"\n",
        "public int add(int a, int b) {\n",
        "    return a - b;\n",
        "}\n",
        "\"\"\"\n",
        "print(generate_summary(java_code))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dkPfVCmt6spa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkPfVCmt6spa",
        "outputId": "0d3dca93-c42c-4098-89ec-5c828a6e3f47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ORIGINAL DOCSTRING\n",
            "Compare the supplied plaintext password to a hashed password.\n",
            "\n",
            "@param   passwd  Plaintext password.\n",
            "@param   hashed  scrypt hashed password.\n",
            "\n",
            "@return true if passwd matches hashed value.\n",
            "\n",
            "TESTING CODE:\n",
            "public static boolean check(String passwd, String hashed) {\n",
            "        try {\n",
            "            String[] parts = hashed.split(\"\\\\$\");\n",
            "\n",
            "            if (parts.length != 5 || !parts[1].equals(\"s0\")) {\n",
            "                throw new IllegalArgumentException(\"Invalid hashed value\");\n",
            "            }\n",
            "\n",
            "            long params = Long.parseLong(parts[2], 16);\n",
            "            byte[] salt = decode(parts[3].toCharArray());\n",
            "            byte[] derived0 = decode(parts[4].toCharArray());\n",
            "\n",
            "            int N = (int) Math.pow(2, params >> 16 & 0xffff);\n",
            "            int r = (int) params >> 8 & 0xff;\n",
            "            int p = (int) params      & 0xff;\n",
            "\n",
            "            byte[] derived1 = SCrypt.scrypt(passwd.getBytes(\"UTF-8\"), salt, N, r, p, 32);\n",
            "\n",
            "            if (derived0.length != derived1.length) return false;\n",
            "\n",
            "            int result = 0;\n",
            "            for (int i = 0; i < derived0.length; i++) {\n",
            "                result |= derived0[i] ^ derived1[i];\n",
            "            }\n",
            "            return result == 0;\n",
            "        } catch (UnsupportedEncodingException e) {\n",
            "            throw new IllegalStateException(\"JVM doesn't support UTF-8?\");\n",
            "        } catch (GeneralSecurityException e) {\n",
            "            throw new IllegalStateException(\"JVM doesn't support SHA1PRNG or HMAC_SHA256?\");\n",
            "        }\n",
            "    }\n",
            "\n",
            "GENERATED SUMMARY:\n",
            "Checks whether the given password matches the given hashed password.\n",
            "\n",
            "@param passwd the password to be checked.\n",
            "@param hashed the hashed password.\n",
            "@return true if the password matches the hashed password.\n"
          ]
        }
      ],
      "source": [
        "test_example = dataset[\"train\"][0]\n",
        "test_code = test_example[\"code\"]\n",
        "print(\"ORIGINAL DOCSTRING\")\n",
        "print(test_example[\"docstring\"])\n",
        "print(\"\\nTESTING CODE:\")\n",
        "print(test_code)\n",
        "\n",
        "generated_summary = generate_summary(test_code)\n",
        "print(\"\\nGENERATED SUMMARY:\")\n",
        "print(generated_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "STe-s8EW-XSj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STe-s8EW-XSj",
        "outputId": "c5a38b4c-5066-4a65-d51f-cfcdf7428697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ORIGINAL DOCSTRING\n",
            "Makes sure the fast-path emits in order.\n",
            "@param value the value to emit or queue up\n",
            "@param delayError if true, errors are delayed until the source has terminated\n",
            "@param disposable the resource to dispose if the drain terminates\n",
            "\n",
            "TESTING CODE:\n",
            "protected final void fastPathOrderedEmit(U value, boolean delayError, Disposable disposable) {\n",
            "        final Observer<? super V> observer = downstream;\n",
            "        final SimplePlainQueue<U> q = queue;\n",
            "\n",
            "        if (wip.get() == 0 && wip.compareAndSet(0, 1)) {\n",
            "            if (q.isEmpty()) {\n",
            "                accept(observer, value);\n",
            "                if (leave(-1) == 0) {\n",
            "                    return;\n",
            "                }\n",
            "            } else {\n",
            "                q.offer(value);\n",
            "            }\n",
            "        } else {\n",
            "            q.offer(value);\n",
            "            if (!enter()) {\n",
            "                return;\n",
            "            }\n",
            "        }\n",
            "        QueueDrainHelper.drainLoop(q, observer, delayError, disposable, this);\n",
            "    }\n",
            "\n",
            "GENERATED SUMMARY:\n",
            "Fast path emit.\n",
            "\n",
            "@param value the value\n",
            "@param delayError the delay error\n",
            "@param disposable the disposable\n"
          ]
        }
      ],
      "source": [
        "test_example = dataset[\"test\"][0]\n",
        "test_code = test_example[\"code\"]\n",
        "print(\"ORIGINAL DOCSTRING\")\n",
        "print(test_example[\"docstring\"])\n",
        "print(\"\\nTESTING CODE:\")\n",
        "print(test_code)\n",
        "\n",
        "generated_summary = generate_summary(test_code)\n",
        "print(\"\\nGENERATED SUMMARY:\")\n",
        "print(generated_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cr6gsllZ_zMj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr6gsllZ_zMj",
        "outputId": "082a67c9-a5ac-4a03-89c1-1de4a904e780"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ORIGINAL DOCSTRING\n",
            "Wraps a SingleSource into a Maybe.\n",
            "\n",
            "<dl>\n",
            "<dt><b>Scheduler:</b></dt>\n",
            "<dd>{@code fromSingle} does not operate by default on a particular {@link Scheduler}.</dd>\n",
            "</dl>\n",
            "@param <T> the target type\n",
            "@param singleSource the SingleSource to convert from\n",
            "@return the new Maybe instance\n",
            "@throws NullPointerException if single is null\n",
            "\n",
            "TESTING CODE:\n",
            "@CheckReturnValue\n",
            "    @NonNull\n",
            "    @SchedulerSupport(SchedulerSupport.NONE)\n",
            "    public static <T> Maybe<T> fromSingle(SingleSource<T> singleSource) {\n",
            "        ObjectHelper.requireNonNull(singleSource, \"singleSource is null\");\n",
            "        return RxJavaPlugins.onAssembly(new MaybeFromSingle<T>(singleSource));\n",
            "    }\n",
            "\n",
            "GENERATED SUMMARY:\n",
            "Construct a Maybe from a SingleSource\n",
            "\n",
            "<pre>\n",
            "{@code\n",
            "\n",
            "import static com.oath.cyclops.reactor.RxJavaPlugins.fromSingle(new SingleSource() {\n",
            "public void run(Object a) {\n",
            "if (a != null) {\n",
            "System.out.println(a);\n",
            "} else {\n",
            "System.out.println(a);\n",
            "}\n",
            "}\n",
            "}\n",
            "}\n",
            "</pre>\n",
            "\n",
            "\n",
            "@param singleSource a SingleSource\n",
            "@param <T> the type of the Maybe\n",
            "@return the Maybe\n"
          ]
        }
      ],
      "source": [
        "test_example = dataset[\"test\"][10]\n",
        "test_code = test_example[\"code\"]\n",
        "print(\"ORIGINAL DOCSTRING\")\n",
        "print(test_example[\"docstring\"])\n",
        "print(\"\\nTESTING CODE:\")\n",
        "print(test_code)\n",
        "\n",
        "generated_summary = generate_summary(test_code)\n",
        "print(\"\\nGENERATED SUMMARY:\")\n",
        "print(generated_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lEBw356UqaYF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689,
          "referenced_widgets": [
            "680f7581615c41ea82183d98400fdde0",
            "d3b71d3726614059a29020eb4edcea94",
            "17d0437d7b5348ec9a463a62285612f8",
            "ffac92baf195469a921c09eff111e94a",
            "eaa3efa1071e4472b31f62343e122b9c",
            "837c7dfa9b344948bc6df99dea4de357",
            "4412c18f709b4d3ab07425462f104c8e",
            "14bd3f6c433b443fb52d2084b0f00e60",
            "d471980c97114e72a0fd80127ecd7b7f",
            "8a49540b169542c490ae88abe891655a",
            "22c80b90a22c4e2184b1ba1dd2e19d42",
            "af4a12995cdc4ad286d3db41bdcd56f0",
            "f81519e811b443a6aed673c979ccbb0b",
            "5cf3d3fb03eb4b2ca373891a3db144e9",
            "2738c053ba39443eb0d85b124fbdab8d",
            "93315276ce6a4eb1b51186b7d290c1dc",
            "dd8ee08108cd476ab26544cc4f318fc8",
            "b48d79d290af414e89d2be19c62a4cf2",
            "5cc81af59d4042ef8773e960c9ce56dd",
            "a750e3095ad04a3c94df1d36b2b115c9",
            "7184989316d14adfa26078f7a55aba3f",
            "f467f5c75c25424db03a40c5ef003e65",
            "a84aa94411f840da9f1f22dd00c676b1",
            "04fc393ea10f4f23b45da010462cfd99",
            "5671b0ad7baa445595610fd93394e8ba",
            "cbb6acd111d34a878209985f59b35191",
            "79b63b6880884b5e944b7933695d2a54",
            "3c0e71575e344776abd4351b662bf776",
            "6724f4517f364d218cce6d65e6efb5a5",
            "ebc691ed7c074238b546c35e21f08f76",
            "c48112a583b548a099c9b87d8075734f",
            "0959a35c26b44ca2af526c159494e31b",
            "67d3961e43104281b29210f56bf984ed",
            "8f7ecac5edab4ac2b27c71f4f6971371",
            "9af9fe44e8014ae89165798102a3f7ae",
            "e14109fc79e1441bb1487ce435dc347d",
            "a4974985fede4531b511055d366ad589",
            "6b6d4b1de53742d5a530ca5030d8ca4f",
            "a821b03ac5594961826093376340995d",
            "ba1ef963643d46b29eb873f68a5a4c16",
            "eb2f027e5bd44d59ab87d23cd03a9ecb",
            "5e6d303c0989444589aa3014777fad12",
            "8aafa72fde91483b966105791ecd4dc6",
            "8cd86b48eedd4e1993bc82c9106c7b00",
            "0caf09032c5e44bda2e9f9179cc66287",
            "347df2b26a294be3b9809b24557b7a4a",
            "29140cf8313f4ccba438af71a9508a28",
            "f8cc21c1d5ea4ee4acf2cda136bf4bba",
            "f46fa2233e3e4215852147d612cadfa3",
            "0c9e070f27594ffda0044c2643b5c3ba",
            "a335de52b4284caba9d9db51700fdf55",
            "84fdeee669e84171b32c80f0f7d332b5",
            "6e889113f8114bcfbd52871099275ab3",
            "8ec87ec53c804096b3b2fd048a3beb09",
            "41d6b299388d46f08a06777829400446",
            "631e1ed300b2482c8b16727c061ad5f4",
            "796041d021bd41c9aaed493e33bab836",
            "897e42c1157041a0a7066f815b82ecf8",
            "728b31ce49ca4d1fbf3ac775efa033df",
            "05f6f8771213407ca88a18a708966bab",
            "ac4f633402c04cce8ca413bc91d1f775",
            "2019a758b79443e8a06f30b029741446",
            "ad833e51a6ac4dcfbd6cc6ae3f60d6b5",
            "619e0d511c5e447eab171a7e90e0bffc",
            "d7354155222c4fafad37f8fec63b3915",
            "a614df89283c44059c62ba9978dcf305",
            "d5269dd3349e4f97a371f4a876859286",
            "d678f15d6acf4c6d816b36a06b77a492",
            "6e32fc59fe044de482ed409d80009c6e",
            "a4b230c2771444fdab4990d8819d03f0",
            "8732bd20aff3414a882d268e72238585",
            "ac2346ddfa8d468b8cddf1157b1e4652",
            "0f90fca5a53f49038353de88776cb118",
            "833f3f01df2b41ab9b2abc1f9c6379ea",
            "965f6d2c77584a0faeaf9a25f462458a",
            "0f933c1971e14cdabcfe88184cdf5d53",
            "7ccfb7f3910645558b57853a74e0dda9",
            "e32202be708c4d26accb3993bde8be57",
            "776245c0f3284c928e79ed0eb8e71e38",
            "b0bb37902ee24751a3bcaa0443cdbac1",
            "4a8f8dc6c09c4184a2b77df744785a92",
            "23e3638256124e86b1219d4eaa002e52",
            "83e51fe6d6bd47f0a7cdfe9ca99af03d",
            "3fb65a55747742348f0aa9b81d017d29",
            "8dced780b57d4f6bbb7728249a837e9f",
            "b93928396bf94e78b6842603b2fb688d",
            "a624f042812d4247b01628a7c206b5c0",
            "d6aef270b1a141fe9dd7dd90bb2d2c37",
            "2dbe3a8cda75495db0317022637118e1",
            "273c7e64eb51454fbecc6e10e35f24b7",
            "562b2c5ec17946fb92e4ca3e9ea46a94",
            "68739fbe57684afa95a95ff2eae1cc18",
            "2d12a5cdc02b4865bf1dbc928e31d5b4",
            "9c44cd64db6e4e09b415f509b0f2ed17",
            "9bc706b2b26349469eabd992863c730b",
            "629379bf8daa43b5912f667b128a856a",
            "08f7961d8fae4d68a425885f0944004f",
            "2f55333d0cd84c31affba8d0acb8ab66",
            "12826ac63c874ef0886947a082168b0e",
            "4e13d61d0aab4bc1bbd3660140ea28b7",
            "c4eb3b7c75a84e93a556d8c610b9318f",
            "37e6f9e3b72b4b6e8d18be71fbd7cce5",
            "80bc2fee90624f13b07187e8ab47e5e6",
            "4652b6424deb445194f05e63ccb7f48b",
            "f80f7ffc0fd248188aecc98469355d2b",
            "cb61287a734f41249860c2e5380bbde2",
            "35057a96989c4e1b8fe25050f1feed86",
            "6d53874cf53b43b392425ceae3784e07",
            "af226e23346f440093c8e48241f88bdc",
            "f945bd0571c64704b849a137d5db07cc",
            "48e273c1856c48489f0b3822d87d280d",
            "95de798ef4bc46b8a2b4fa7671f96d32",
            "d96d0785fa494c92bf89111dba8d27ec",
            "a4f5b07cb1db4ef6bacc2de223069060",
            "7810cb58297846498644a2b8cf050d28",
            "d85c6c5926ed4d3bb9bec0585cc7d8b2",
            "55a8c2add8a04cbdad78276bd09ef0c7",
            "0a56e8d7e962457a8a80994609ababae",
            "1f189583571e4fc6a7e6df7b256597df",
            "f9aa8db0b4ae4b72a71cbab0564818d3",
            "d3eca68097f449a8ba8c4c6463c654c8"
          ]
        },
        "id": "lEBw356UqaYF",
        "outputId": "c72c2c15-7334-4242-9ebb-d228557737cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "680f7581615c41ea82183d98400fdde0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af4a12995cdc4ad286d3db41bdcd56f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a84aa94411f840da9f1f22dd00c676b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f7ecac5edab4ac2b27c71f4f6971371",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0caf09032c5e44bda2e9f9179cc66287",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "631e1ed300b2482c8b16727c061ad5f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5269dd3349e4f97a371f4a876859286",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e32202be708c4d26accb3993bde8be57",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2dbe3a8cda75495db0317022637118e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e13d61d0aab4bc1bbd3660140ea28b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48e273c1856c48489f0b3822d87d280d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Version\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"beam_repetition\",\n          \"baseline_beam\",\n          \"sampling_temp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ROUGE-1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03471402214283637,\n        \"min\": 0.4059,\n        \"max\": 0.4677,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.4677,\n          0.4094,\n          0.4059\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ROUGE-2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04095351022806227,\n        \"min\": 0.1746,\n        \"max\": 0.2558,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.2558,\n          0.2245,\n          0.1746\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ROUGE-L\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04348750778480336,\n        \"min\": 0.3348,\n        \"max\": 0.4205,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.4205,\n          0.3648,\n          0.3348\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BLEU\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.020977209855777616,\n        \"min\": 0.1224,\n        \"max\": 0.1643,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.1643,\n          0.1415,\n          0.1224\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BERTScore\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.008827419403955683,\n        \"min\": 0.8667,\n        \"max\": 0.884,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.884,\n          0.8667,\n          0.8723\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Exact Match\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.002872861523522032,\n        \"min\": 0.0014,\n        \"max\": 0.0068,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0068,\n          0.0058,\n          0.0014\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Avg Token Repetition\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18301396121607774,\n        \"min\": 0.2246,\n        \"max\": 0.5815,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.4734,\n          0.5815,\n          0.2246\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "results_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-937a37c4-13ce-49e3-87a7-3b6d40760033\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Version</th>\n",
              "      <th>ROUGE-1</th>\n",
              "      <th>ROUGE-2</th>\n",
              "      <th>ROUGE-L</th>\n",
              "      <th>BLEU</th>\n",
              "      <th>BERTScore</th>\n",
              "      <th>Exact Match</th>\n",
              "      <th>Avg Token Repetition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>beam_repetition</td>\n",
              "      <td>0.4677</td>\n",
              "      <td>0.2558</td>\n",
              "      <td>0.4205</td>\n",
              "      <td>0.1643</td>\n",
              "      <td>0.8840</td>\n",
              "      <td>0.0068</td>\n",
              "      <td>0.4734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>baseline_beam</td>\n",
              "      <td>0.4094</td>\n",
              "      <td>0.2245</td>\n",
              "      <td>0.3648</td>\n",
              "      <td>0.1415</td>\n",
              "      <td>0.8667</td>\n",
              "      <td>0.0058</td>\n",
              "      <td>0.5815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sampling_temp</td>\n",
              "      <td>0.4059</td>\n",
              "      <td>0.1746</td>\n",
              "      <td>0.3348</td>\n",
              "      <td>0.1224</td>\n",
              "      <td>0.8723</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.2246</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-937a37c4-13ce-49e3-87a7-3b6d40760033')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-937a37c4-13ce-49e3-87a7-3b6d40760033 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-937a37c4-13ce-49e3-87a7-3b6d40760033');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d0c33929-30a6-4317-b3f1-df46d21f8771\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d0c33929-30a6-4317-b3f1-df46d21f8771')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d0c33929-30a6-4317-b3f1-df46d21f8771 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_5fcaa312-68a0-4857-ba54-525b339aab57\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5fcaa312-68a0-4857-ba54-525b339aab57 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "           Version  ROUGE-1  ROUGE-2  ROUGE-L    BLEU  BERTScore  Exact Match  \\\n",
              "1  beam_repetition   0.4677   0.2558   0.4205  0.1643     0.8840       0.0068   \n",
              "0    baseline_beam   0.4094   0.2245   0.3648  0.1415     0.8667       0.0058   \n",
              "2    sampling_temp   0.4059   0.1746   0.3348  0.1224     0.8723       0.0014   \n",
              "\n",
              "   Avg Token Repetition  \n",
              "1                0.4734  \n",
              "0                0.5815  \n",
              "2                0.2246  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 🧪 Analysis Script\n",
        "import pandas as pd\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from collections import Counter\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# File paths for each decoding strategy\n",
        "files = {\n",
        "    \"baseline_beam\": \"/content/drive/MyDrive/codet5_val_predictions.csv\",\n",
        "    \"beam_repetition\": \"/content/drive/MyDrive/codet5_val_beam_repetition.csv\",\n",
        "    \"sampling_temp\": \"/content/drive/MyDrive/codet5_val_sampling_temp.csv\"\n",
        "}\n",
        "\n",
        "# Load evaluation metrics\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "\n",
        "# Function to compute average token repetition per summary\n",
        "def avg_token_repetition(predictions):\n",
        "    rep_counts = []\n",
        "    for text in predictions:\n",
        "        tokens = text.strip().split()\n",
        "        counts = Counter(tokens)\n",
        "        repeated_tokens = sum(v for v in counts.values() if v > 1)\n",
        "        rep_counts.append(repeated_tokens / max(1, len(tokens)))\n",
        "    return np.mean(rep_counts)\n",
        "\n",
        "# Run metrics for each version\n",
        "all_results = []\n",
        "\n",
        "for name, path in files.items():\n",
        "    if not os.path.exists(path):\n",
        "        print(f\" File not found: {path}\")\n",
        "        continue\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "    predictions = df[\"predicted_summary\"].astype(str).tolist()\n",
        "    references = df[\"gold_summary\"].astype(str).tolist()\n",
        "\n",
        "    # Compute metrics\n",
        "    rouge_scores = rouge.compute(predictions=predictions, references=references, use_stemmer=True)\n",
        "    bleu_score = bleu.compute(\n",
        "        predictions=predictions,\n",
        "        references=references\n",
        "    )\n",
        "    bert_score = bertscore.compute(predictions=predictions, references=references, lang=\"en\", device=\"cuda\")\n",
        "    exact_match = sum(p.strip() == r.strip() for p, r in zip(predictions, references)) / len(references)\n",
        "    repetition = avg_token_repetition(predictions)\n",
        "\n",
        "    all_results.append({\n",
        "        \"Version\": name,\n",
        "        \"ROUGE-1\": round(rouge_scores[\"rouge1\"], 4),\n",
        "        \"ROUGE-2\": round(rouge_scores[\"rouge2\"], 4),\n",
        "        \"ROUGE-L\": round(rouge_scores[\"rougeL\"], 4),\n",
        "        \"BLEU\": round(bleu_score[\"bleu\"], 4),\n",
        "        \"BERTScore\": round(np.mean(bert_score[\"f1\"]), 4),\n",
        "        \"Exact Match\": round(exact_match, 4),\n",
        "        \"Avg Token Repetition\": round(repetition, 4)\n",
        "    })\n",
        "\n",
        "# Display comparison table\n",
        "results_df = pd.DataFrame(all_results)\n",
        "results_df = results_df.sort_values(\"ROUGE-L\", ascending=False)\n",
        "display(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BhnM6reJg3ol",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571,
          "referenced_widgets": [
            "4e01053acdc8478bbc4b12ea42ee8102",
            "2777c4570ad14c4e8cf1b21bb2bd17ba",
            "0c5a876fdc4343bb8398dae2a50c6554",
            "0e6d1c58791941e0a120394e5a5b88bd",
            "a2800859bf464b548895d5116cbde173",
            "e7832094aa564accb2722f09c14c248f",
            "e96c17873f844a27a201ce35f9137f14",
            "86e876251aee4a54b7c4976b79081e47",
            "dcf7902b325b4b1392c4405291296e05",
            "14bd84cae05b430a82309da349ef9ed7",
            "0686a401dda54fc48f3a8f394dcb1158",
            "38ad1413d4294fe49ac556fa6fd3c20e",
            "f12adb3c0c6c4bcc8fb14b757888d8b2",
            "b0bd3818258747528e90aa1f19749c7a",
            "3ff72df097e9474aaf18d21ba2ffe6e9",
            "e695450e6013472794707995b9fdd5a5",
            "4ecfc7585a964cb09b6dbb4ca1e10026",
            "c749002d24454330b771d24084036968",
            "e0527a41fdf5442baff5ffe1f956b375",
            "9dc08c4f654c4a28b6dc160353d6ea12",
            "d0b576e079c34f6dae7e1261289ec506",
            "adcfbd8a04f5448aac8589937182d700",
            "7639068251b942a3a77a70623f2f4c07",
            "049d5a447a1149d5bbd03cf3e5a2d9c6",
            "8b5168038e7d47df9b0d9f25c7ae28e0",
            "771b32cbb6854c35bd4d32d817972d2a",
            "231b540a90234cdf8016b71bfb99c64d",
            "9127560e0c66490d89eedf68551850f4",
            "83979d0ff366418695f9d351ae9ca75c",
            "3d290815e07e4bd5bac9e28e03e3b102",
            "051328ec5c164e44ad7b4b3ebd973489",
            "83c1f175c8d04b9baa6005858cc6ac55",
            "52e101e2cc1b4d1d84c015390b45f54d",
            "853cf845ff194d11919bd941c2be120b",
            "55bace55adcc43e0881bf5e32635a941",
            "6f27dd75c43246a684227b12da983b1c",
            "18f5ae62afba40d5bdd8c2aadc95d2d7",
            "effaa7096eb04c6ca8588f72821b6425",
            "80dfff114ca248ad9ef96bf4ca8120d7",
            "50ef3c59050f41ee95b2e40ba99023cd",
            "2110206a3ae347c5878d7dcb1ab2c783",
            "e0c110494be849e3981357c980df832d",
            "6de8211cbda74d5e8ea550abfd2ec64c",
            "27a53d762fc449229a09b7664b635ea9",
            "ddce6d5a721b44878a13f0b2d3b46426",
            "69e50eae64254b7d924899609aa012b1",
            "2b373dda909d479787b0f4231bf7d8ac",
            "953bdc77a8104f7ea3bae710cef269bf",
            "ab04d951da984fb7b707d0be49428a27",
            "9a4ce6002d614be7b2123d4598a49879",
            "7c061165b8574a09bb4f3056d8921714",
            "4ed2bebb6fe049b1a357f550f76ba313",
            "3babdefdf3a64d08b04d54ee5197a620",
            "e18c0d50238a4c69b63253fb73636710",
            "1d2bb0a5b5544ef1a34592ffaae8f3e5",
            "a5047b4a9725404892f6b1cdbdc04e66",
            "4ebd619aa2464a7db708e182da696bcb",
            "c936ec08491a4cb4bfc5d3b0db0866cb",
            "c531a5cd19df4679910737bee3847855",
            "3546369c2eb14de2941cc6c796bf28ab",
            "a31403562364498d86dd9d60cb9f5ad3",
            "522f35bc6ff1452fbb5dffb382574b79",
            "ef641fa33d7f41dbb137acdd74815a13",
            "75fd1060496d410cb7a4c95bfead7772",
            "606f8f502db940228a45a576c92825a8",
            "4957230ce30748dfaa9e5ee596609efe",
            "d0dd54dddcc34fc4a828bab626e62e2c",
            "e1f55f6afec04635b73e70d77c857ac0",
            "9f392e0333024570a118b499767c91b8",
            "4708415c02a5436a995af0314745cd36",
            "011eab2cf196484694e8f104eeda29e7",
            "38347290a10e471eab5844be4c29a124",
            "6a0bb03d9623458188f5cb3968cedefd",
            "3abfd9f04dd649e385e287c32e930e55",
            "b9c1ef08785a46d5bdfaf40e8d0a22c6",
            "f54c1d938d3c4a4999e3688d5abd7f7a",
            "449dd4a58ece48e29140d384602f8d47",
            "8295c2945a044eb3ab45aaca9191f3a6",
            "e9fcab5138694df29212e3a087655119",
            "64d35488f44c41e2aef6cf961a8a2ad3",
            "d35af5c82b184881bbb8792817806d9d",
            "71cb61653f7f427499ea57e1979d79c3",
            "6d381dafce934205a52ab427dcaaa9a4",
            "99d735b1cfe6491ebb4fcd2949809ed7",
            "b9302c2e84c74904b890748090a46d51",
            "68a99364d40740aba27494bd87da3c9e",
            "aed7784262924c948171b0fb6104ef8e",
            "05c403353fda4a978d2f4b1ea06a6e22"
          ]
        },
        "id": "BhnM6reJg3ol",
        "outputId": "b501a7f8-262e-4f45-98a3-7bdd5d02ff00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e01053acdc8478bbc4b12ea42ee8102",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/26.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38ad1413d4294fe49ac556fa6fd3c20e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/141M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7639068251b942a3a77a70623f2f4c07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "validation-00000-of-00001.parquet:   0%|          | 0.00/4.25M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "853cf845ff194d11919bd941c2be120b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/9.38M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddce6d5a721b44878a13f0b2d3b46426",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/164923 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5047b4a9725404892f6b1cdbdc04e66",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/5183 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0dd54dddcc34fc4a828bab626e62e2c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/10955 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8295c2945a044eb3ab45aaca9191f3a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5183 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rGenerating (Sampling + Temp):   0%|          | 0/324 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "Generating (Sampling + Temp):  15%|█▌        | 50/324 [01:31<08:20,  1.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving at batch 50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (Sampling + Temp):  31%|███       | 100/324 [03:04<07:17,  1.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving at batch 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (Sampling + Temp):  46%|████▋     | 150/324 [04:38<05:31,  1.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving at batch 150\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (Sampling + Temp):  62%|██████▏   | 200/324 [06:12<03:41,  1.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving at batch 200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (Sampling + Temp):  77%|███████▋  | 250/324 [07:44<02:21,  1.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving at batch 250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (Sampling + Temp):  93%|█████████▎| 300/324 [09:19<00:48,  2.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving at batch 300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating (Sampling + Temp): 100%|██████████| 324/324 [10:03<00:00,  1.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving at batch 324\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# SCRIPT 2: Sampling with Temperature + Top-k\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Configs\n",
        "model_dir = \"/content/drive/MyDrive/codet5_checkpoints/checkpoint-46386\"\n",
        "save_path = \"/content/drive/MyDrive/codet5_val_sampling_temp.csv\"\n",
        "save_every = 50\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.eval()\n",
        "\n",
        "dataset = load_dataset(\"code_x_glue_ct_code_to_text\", \"java\")\n",
        "val_data = dataset[\"validation\"]\n",
        "gold_summaries = val_data[\"docstring\"]\n",
        "\n",
        "def tokenize_fn(example):\n",
        "    return tokenizer(example[\"code\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "val_tokenized = val_data.map(tokenize_fn, batched=True)\n",
        "val_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "val_loader = DataLoader(val_tokenized, batch_size=16)\n",
        "\n",
        "generated_summaries = []\n",
        "start_batch = 0\n",
        "\n",
        "if os.path.exists(save_path):\n",
        "    df_existing = pd.read_csv(save_path)\n",
        "    generated_summaries = df_existing[\"predicted_summary\"].tolist()\n",
        "    start_batch = len(generated_summaries) // 16\n",
        "    print(f\"Resuming from batch {start_batch} (already {len(generated_summaries)} predictions)\")\n",
        "\n",
        "for i, batch in enumerate(tqdm(val_loader, desc=\"Generating (Sampling + Temp)\")):\n",
        "    if i < start_batch:\n",
        "        continue\n",
        "\n",
        "    input_ids = batch[\"input_ids\"].to(model.device)\n",
        "    attention_mask = batch[\"attention_mask\"].to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            temperature=0.7,\n",
        "            max_new_tokens=64,\n",
        "            repetition_penalty=1.2,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    generated_summaries.extend(decoded)\n",
        "\n",
        "    if (i + 1) % save_every == 0 or (i + 1) == len(val_loader):\n",
        "        print(f\"Saving at batch {i + 1}\")\n",
        "        df = pd.DataFrame({\n",
        "            \"gold_summary\": gold_summaries[:len(generated_summaries)],\n",
        "            \"predicted_summary\": generated_summaries\n",
        "        })\n",
        "        df.to_csv(save_path, index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "nlpProject",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
